#!/bin/bash
# 
# ----------------------------------------
# Exercise 2: Other ways to infer stuff
# ----------------------------------------
# Steps:
#   1) Running fastsimcoal to estimate the parameters of a two population model
#   2) GWAS at low coverage
#   3) Inferring heterozygosity from low covrage genomes
# 
# To see the individual steps, type './ex2.sh STEP', where STEP is the number of the desired step.
# 
# IMPORTANT: To have easy access to the executables, add the LOCATION/bin folder of these exercises to your PATH. Here LOCATION corresponds to the place you copied the exercises to. This is done by executing
# 
# > PATH="${PATH}:LOCATION/bin"
# 
# Then, you also need to make all the files in the /bin folder executable. You can do this by executing the following command inside your bin folder:
# 
# > chmod +x *
# 

#1 
#1 Step 1: Running fastsimcoal to estimate the parameters of a two population model
#1 ------------------------------------------------------------------------------------------
#1 - fastcimcoal also implements a composute likelihood framework to infer demographic parameters. The usage is very similar to ABCtoolbox, so we will here just discuss one example. But to make it more interesting, we will try to infer parameters of a model with two populations.
#1 - Just as when doing simulations, the model is specified in a specific input file. The relevant file in 'twopop.tpl', and the corresponding estimation file is 'twopo.est'. Have a look at those files to understand how models and parameter ranges are specified (NOTE: fastsicoal implements a maximum likelihood approach, so these are not priors!).
#1 - The genetic data is provided as a joint SFS given in file 'twopop_jointDAFpop1_0.obs'. Have a look at this file too. Note that it is recommended to use ANGSD to infer an SFS from your real data.
#1 - Use the following command line (after making sure you understand it!) to infer the parameters of this model. This will a few minutes to complete. 
#1 
#1 >  fsc25221 -t twopop.tpl -e twopop.est -n 10000 -N 10000 -d -M 0.01 -l20 -L 100
#1 
#1 - All output files are written into the folder 'twopop'. Among them, you will find a file with the ending '.bestlhoods', which contains the maximum likelihood estimates obtained with fastsimcoal. Do they match the parameters I used to simulate? I used POPSIZE_1 = 1000, POPSIZE_2 = 10000, ANCESTRALSIZE = 500 and TDIV = 1000.
#  - Since fastsimcoal implements a heuristic search algorithm, you may get slightly different results in each run. Let's therefore rerun the estimation. Do you get similar estimates?
#1 


#2 
#2 Step 2: GWAS at low coverage
#2 ------------------------------
#2 - We will next try to see how much we can actually learn from samples that were sequenced a very low coverage. As a first example, you will now simulate genetic data for several individuals at various coverages and then test if you can accurately infer which SNPS are associated with a trait.
#2 - !!! ATTENTION - SPOILER ALERT !!! Yes, you will learn about GWAS only tomorrow, this is just to play around a little bit...
#2 - To do both simulations as well as inference we will use a mutated marine organism: spaunge! This program is precompiled and ready to use in the bin folder.
#2 - First we will use this program to simulate genetic data. The pipeline consist of the following steps:
#2     * simulating SAM files with spaunge
#2     * using samtools to turn these SAM files into BAM files
#2     * using ANGSD to calculate genotype likelihoods to these individuals
#2     * using spaunge to check for associations
#2 
#2 - Since these steps involve a lot of nasty loops, I provide a script called 'testAssociations.sh' in the bin folder. Have a look at this script to figure out how it works. Have a particularly careful look at the command line to simulate data and locat ethe arguments 'ind', 'coverage' and 'explVar', as you will be playing around with those. The meanings of these commands are: number of individuals to be simulated, coverage per individual, and the proportion of the phenotypic variance explained by the SNP affecting the phenotype.
#2 - Now let's run the script:
#2 
#2 > ./testAssociations.sh
#2 
#2 - You will now find plenty of files in your folder: all the simulated SAM and BAM files, the genotype likelihood file generated by ANGSD ('spaunge_ANGSD.beagle.gz') as well as the file 'spaunge_ANGSD.beagle_additive_associations_Phenotype0.txt', which summarizes the association results. Have a look at those files to see how the work. To check out zipped files you can use zcat:
#2 
#2 > zcat spaunge_ANGSD.beagle.gz | less
#2 
#2 - Now let's check if the correct mutation was identified as being associated. When simulating data, spaunge also writes a file called 'spaunge_snps.txt' that list all simulated SNPs. Cat this file to figure out which of the 10 simulated SNPs actually affects the phenotype (the one with a 1 in the Phenotype0 column).
#2 - No check out the file 'spaunge_ANGSD.beagle_additive_associations_Phenotype0.txt' and investiagte which SNPs were found to be significantly associated with the phenotype. Note: the last column lists the -log10(p-value) of the association test. So a value of 2 corresponds to 0.01. Was the correct SNP identified?
#2 - Now rerun the whole procedure for 500 individuals at coverage 2, so effectively reducing sequencing costs by a factor of 2 (ten times less coverage, five times as many individuals). Does that improve your power to find the association? And how about at 1x coverage?


#3 
#3 Step 3: Inferring heterozygosity from low coverage genomes
#3 -------------------------------------------------------------
#3 - We will next try to infer genome wide genetic diversity of an individual (heterozygosity) in 1Mb windows. To do so, you will use data from an ancient (8000 years old) human sample from Greece we used a recent study (Bar8). The data provided just concerns the first 10Mb of chromosome 1.
#3 - As a first step, you will just use one of our programs calles estimHet to infer heterozygoisty in 1Mb windows from this data. You will find a precompiled version of estimHet in the bin folder. The command line to be used is then:
#3 
#3 > estimHet bam=Bar8_chr1_10Mb_ok.bam verbose window=1000000
#3 
#3 NOTE: you need to load the current g++ module in order to run estimHet. You can do this using
#3 
#3 > module load gcc/4.8.2 
#3 
#3 - The resulting heterozygoisty (theta) estimates are found in the output file 'Bar8_chr1_10Mb_theta_estimates.txt'. Do these estimates make sense? Note that the expected heterozygosity in humans is about 10^-3.
#3 - One reason for why these estimates might be so high is that ancient DNA is affected by post-mortem damage. So let's try to see what kind of estimates we will get when taken PMD into account. Estimates of PMD rates are given in the file 'Bar8_pmd.txt', which you can provide to estimHet as follows:
#3 
#3 > estimHet bam=Bar8_chr1_10Mb_ok.bam verbose window=1000000 pmdFile=Bar8_pmd.txt out=with_PMD
#3 
#3 - How did this affect the estimates? One additional problem might be that the quality scores given by the sequencing machine are off. We have already estimated parameters to recalibrate this data, all found in the files named 'Bar8_BQSR_*_Table.txt'. You can provide this information to estimHet as follows:
#3 
#3 > estimHet bam=Bar8_chr1_10Mb_ok.bam verbose window=1000000 pmdFile=Bar8_pmd.txt BQSRQuality=Bar8_BQSR_ReadGroup_Quality_Table.txt BQSRPosition=Bar8_BQSR_ReadGroup_Position_Table.txt BQSRPositionReverse=Bar8_BQSR_ReadGroup_Position_Reverse_Table.txt BQSRContext=Bar8_BQSR_ReadGroup_Context_Table.txt out=with_PMD_BQSR
#3 
#3 - How are the estimates now? What we learn from this -> when working with methods that take quality scores into account, we have to make sure they are correct!
#3
#3 - Now let's check how much our results are affected by covrage. For this we will use estimhet to downsample the bam file to much lower coverage and compare our estimates. But first let's calculate the actual coverage from the bamfile:
#3 
#3 > estimHet task=coverage bam=Bar8_chr1_10Mb.bam verbose 
#3 
#3 - To downsample to, say, 1.5x coverage, you can use estimHet as follows, and then samtools to create an index file
#3 
#3 > estimHet task=downsample bam=Bar8_chr1_10Mb.bam verbose prob=0.25
#3 > samtools  index Bar8_chr1_10Mb_downsampled0.25.bam
#3 
#3 - Now rerun the heterozygosity estimation. Do you get vastly different results?
#3 - To see a more striking effect of the recalibration, reestimate heterozygosity at low coverage without recalibration. Conclusion: the lower the coverage, the more import is recalibration!
#3 


LOC=$(which ./ex2.sh)
PATTERN=#$1[[:blank:]]
grep "$PATTERN" $LOC | awk '{$1=""; print $0}'
